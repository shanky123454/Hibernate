<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Manage Kafka clusters with AKHQ and AMQ streams</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/26/manage-kafka-clusters-akhq-and-amq-streams" /><author><name>Rogerio Santos</name></author><id>03c3e30e-2987-41f7-9c98-5197fc74dc70</id><updated>2023-07-26T07:00:00Z</updated><published>2023-07-26T07:00:00Z</published><summary type="html">&lt;p&gt;A Graphical User Interface, or GUI, is highly important for &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; administrators and developers. Having the ability to visualize and interact with topics or make changes quickly can save a significant amount of time. While Red Hat's &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;AMQ&lt;/a&gt; streams Operator is often considered a lightweight GUI, more advanced and detailed information can only be obtained using command-line tools such as &lt;code&gt;kafka-consumer-groups.sh&lt;/code&gt;, &lt;code&gt;kafka-acls.sh&lt;/code&gt;, etc.&lt;/p&gt; &lt;p&gt;In most of the customer implementations of Kafka clusters I have encountered, the most common question is: "Where is the web console?"&lt;/p&gt; &lt;p&gt;The answer to this question is simple: AMQ streams does not have a built-in GUI. However, there are many free and paid third-party options available that are fully compatible with AMQ streams. Among dozens of tools, one particular tool caught my attention: &lt;a href="https://akhq.io/"&gt;AKHQ&lt;/a&gt;. In this article, I will demonstrate how to deploy AKHQ on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; 4 and connect it to AMQ streams.&lt;/p&gt; &lt;h2&gt;The Kafka cluster&lt;/h2&gt; &lt;p&gt;The example Kafka cluster that I will use for this article has the following characteristics:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Authentication mechanism: &lt;code&gt;SCRAM-SHA-512&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Protocol: &lt;code&gt;SASL_SSL&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Sasl.jaas.config: &lt;code&gt;org.apache.kafka.common.security.scram.ScramLoginModule required username="akhq" password="NmfwVqrNZKyy";&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Namespace: &lt;code&gt;amq-streams-lab&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Kafka version: 3.3.1 with operator version v2.3.0-3&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;I will install AKHQ in the same namespace as AMQ streams because I want it to take over the GUI role solely for this Kafka installation. However, it would be fine to install it in a different namespace. One of the features of AKHQ is its support for configuring multiple clusters, making it a convenient central GUI for managing multiple Kafka clusters.&lt;/p&gt; &lt;h2&gt;Prepare the AKHQ package&lt;/h2&gt; &lt;p&gt;To begin, let's clone AKHQ from the following address: &lt;a href="https://github.com/tchiotludo/akhq.git"&gt;https://github.com/tchiotludo/akhq.git&lt;/a&gt;. The example shown in this article uses AKHQ version 0.24.0.&lt;/p&gt; &lt;p&gt;We'll perform the installation using Helm. Within the cloned project, there is a folder named &lt;code&gt;helm/akhq&lt;/code&gt;&lt;em&gt; &lt;/em&gt;containing everything necessary for deployment on OpenShift 4.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/tchiotludo/akhq.git cd ./akhq/helm/akhq&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Within the folder, you will find a file named &lt;code&gt;values.yaml.&lt;/code&gt; Edit this file and fill in the Kafka cluster connection parameters. Locate the &lt;code&gt;secrets {}&lt;/code&gt; property, remove the comments, and fill it out as shown in the example below:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;secrets: akhq: connections: amqstreams-lab: properties: bootstrap.servers: "amqstreams-lab-kafka-bootstrap.amq-streams-lab.svc.cluster.local:9095" security.protocol: SASL_SSL sasl.mechanism: SCRAM-SHA-512 sasl.jaas.config: org.apache.kafka.common.security.scram.ScramLoginModule required username="akhq" password="NmfwVqrNZKyy"; ssl.truststore.location: /opt/kafka/cluster-ca-certs/ca.p12 ssl.truststore.password: bnnZ0bY9L79i&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;While configuring the &lt;code&gt;ssl.truststore.password&lt;/code&gt; and &lt;code&gt;ssl.truststore.location&lt;/code&gt; properties, it's essential to remember that these values will be retrieved from the Kafka cluster's certificate secret. Further clarity on this will be provided during the configuration of &lt;code&gt;extraVolumes&lt;/code&gt; and &lt;code&gt;extraVolumeMount&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Another necessary change is to adjust the service port. Locate the &lt;code&gt;service&lt;/code&gt; property and modify the value from &lt;code&gt;80&lt;/code&gt; to &lt;code&gt;8080&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;service: enabled: true type: ClusterIP port: 8080 managementPort: 28081 #httpNodePort: 32551 labels: {} annotations: # cloud.google.com/load-balancer-type: "Internal"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm utilizing a Kafka cluster that requires a certificate for authentication. Since I'm deploying AKHQ in the same namespace as AMQ streams, I'll configure AKHQ to retrieve the cluster certificate from the &lt;code&gt;secret&lt;/code&gt; associated with the cluster. Locate the &lt;code&gt;extraVolumeMounts&lt;/code&gt; and &lt;code&gt;extraVolumes&lt;/code&gt; properties and populate them as demonstrated below.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;# Any extra volumes to define for the pod (like keystore/truststore) extraVolumes: - name: cluster-ca-cert secret: secretName: amqstreams-lab-cluster-ca-cert defaultMode: 420 # Any extra volume mounts to define for the akhq container extraVolumeMounts: - name: cluster-ca-cert mountPath: /opt/kafka/cluster-ca-certs/ca.p12 subPath: ca.p12 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The password required for the &lt;code&gt;ssl.truststore.password&lt;/code&gt; property, mentioned earlier in this article, can be retrieved also from the secret named &lt;code&gt;amqstreams-lab-cluster-ca-cert&lt;/code&gt;. In each Kafka cluster, there exists a secret containing both its certificate and password, and the naming convention for this secret follows the structure &lt;code&gt;&lt;CLUSTER NAME&gt;.cluster-ca-cert&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The final essential configuration is to define the route for accessing AKHQ. In the &lt;code&gt;values.yaml&lt;/code&gt; file, locate the &lt;code&gt;ingress&lt;/code&gt; property, and populate it as shown below.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;ingress: enabled: true ingressClassName: "" annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: "true" paths: - / hosts: - akhq-amq-streams-lab.apps-crc.testing tls: [] # - secretName: akhq-tls # hosts: # - akhq.demo.com&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this example, the host is composed of the following structure: &lt;code&gt;akhq + &lt;namespace name&gt; + &lt;Openshift host&gt;&lt;/code&gt;. You have the flexibility to include annotations, certificates, or any valid domain.&lt;/p&gt; &lt;h2&gt;Deploy and run&lt;/h2&gt; &lt;p&gt;To deploy AKHQ, I will use the Helm install &lt;name&gt; command, as shown below. (Note: I used Helm version 3.11. Check the syntax of &lt;code&gt;install&lt;/code&gt; in other versions.)&lt;/p&gt; &lt;pre&gt; &lt;code&gt;oc project amq-streams-lab helm install akhq-amqstreams  .&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result of executing this command will be the following artifacts within the namespace:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;1 pod has been created with a name similar to &lt;code&gt;akhq-amqstreams-XXXXX&lt;/code&gt;&lt;/li&gt; &lt;li&gt;1  deployment created with the name &lt;code&gt;akhq-amqstreams&lt;/code&gt;&lt;/li&gt; &lt;li&gt;1 secret created with the name &lt;code&gt;akhq-amqstreams-secrets&lt;/code&gt;&lt;/li&gt; &lt;li&gt;1 ConfigMap created with the name &lt;code&gt;akhq-amqstreams&lt;/code&gt;&lt;/li&gt; &lt;li&gt;1 replicaSet created with a name similar to &lt;code&gt;akhq-amqstreams-XXXXX&lt;/code&gt;&lt;/li&gt; &lt;li&gt;1 service created with the name &lt;code&gt;akhq-amqstreams&lt;/code&gt;&lt;/li&gt; &lt;li&gt;1 NetworkPolicy created with a name &lt;code&gt;akhq-amqstreams&lt;/code&gt;&lt;/li&gt; &lt;li&gt;1 route created with a name similar to &lt;code&gt;akhq-amqstreams-XXXXX&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To access AHKQ, use the route created during the installation. You will see a screen similar to the one shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/akhq1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/akhq1.png?itok=Q3J-vO1U" width="1440" height="766" alt="The AHKQ dashboard." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The AHKQ dashboard.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Now, you can navigate through the tool and enjoy the experience.&lt;/p&gt; &lt;h2&gt;Final considerations&lt;/h2&gt; &lt;p&gt;AKHQ is an excellent complement to AMQ streams. In this article, I provided a quick start guide. However, you can enhance this installation by incorporating additional features such as implementing a login using Red Hat's single sign-on tool, creating a service account, or scaling up the number of pods. You can even customize the appearance by adding a new logo to enhance the user experience further.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/26/manage-kafka-clusters-akhq-and-amq-streams" title="Manage Kafka clusters with AKHQ and AMQ streams"&gt;Manage Kafka clusters with AKHQ and AMQ streams&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Rogerio Santos</dc:creator><dc:date>2023-07-26T07:00:00Z</dc:date></entry><entry><title>End-to-end testing with self-hosted runners in GitHub Actions</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/25/end-end-testing-self-hosted-runners-github-actions" /><author><name>Jianzhu Zhang, Andrew Kiselev, Daniel Kostecki</name></author><id>86833e73-bbec-48eb-8325-912250049ea6</id><updated>2023-07-25T07:00:00Z</updated><published>2023-07-25T07:00:00Z</published><summary type="html">&lt;p&gt;Accelerating the software development life cycle while ensuring the quality and performance of applications is a challenging task. GitHub Actions makes it easy to automate all required CI software workflows for your GitHub repository.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/actions/runner"&gt;GitHub Actions Runner&lt;/a&gt; is an application that runs a job from a GitHub Actions workflow. GitHub Actions also provides a self-hosted runner that allows you to run &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous integration (CI)&lt;/a&gt; tests that require actual hardware. End-to-end testing (E2E testing) is a popular methodology to test an application's functionality and performance under real-life conditions. Still, it often demands actual hardware, rendering it infeasible to run on the public cloud.&lt;/p&gt; &lt;p&gt;In this article, we'll delve into our experience performing E2E testing for an open source project on-premises using a containerized self-hosted runner. The self-hosted runner &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; image used in this tutorial is available for download from the &lt;a href="quay.io/gitaction/runner"&gt;quay.io registry&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Self-hosted runner container on Red Hat Enterprise Linux&lt;/h2&gt; &lt;p&gt;First, we'll create a self-hosted runner container on &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL).&lt;/p&gt; &lt;h3&gt;Build and download the containerized runner image&lt;/h3&gt; &lt;p&gt;The whole procedure is covered in &lt;a href="https://github.com/redhat-eets/gitaction"&gt;https://github.com/redhat-eets/gitaction&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To build the containerized runner for a given runner version, enter the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman build --build-arg RUNNER_VERSION=2.301.1 --tag quay.io/gitaction/runner:2.301.1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To see what runner releases are available to use for &lt;code&gt;RUNNER_VERSION&lt;/code&gt;, check on&lt;a href="https://github.com/actions/runner/releases"&gt; https://github.com/actions/runner/releases&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Alternatively, you can download a specific self-hosted runner container image for the 2.301.1 release:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/gitaction/runner:2.301.1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The GitHub runner will check if a newer version is available on startup. It will self-update and restart with the latest version. However, it is worth using the latest version for the container image. Note that the runner's self-update takes time and may not always be successful.&lt;/p&gt; &lt;h3&gt;GitHub token protection&lt;/h3&gt; &lt;p&gt;In order to generate a registration token, the container requires you to enter a &lt;a href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token"&gt;GitHub personal access token&lt;/a&gt; (PAT) when starting. The PAT has to belong to the target repository owner for the container to register successfully.&lt;/p&gt; &lt;p&gt;From a security perspective, using the PAT directly with the Podman command is not a good idea. Instead, a Podman secret should be created for the PAT:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;echo "your github access token" &gt; token &amp;&amp; podman secret create github_token token &amp;&amp; rm -rf token&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the above step, &lt;code&gt;github_token&lt;/code&gt; must be used as the secret name, as this is the default secret filename that the script inside the container will look for. If you want to choose a different secret name, you can use the environment variable &lt;code&gt;GH_TOKEN_PATH&lt;/code&gt; to specify the secret file path when running Podman to start the container.&lt;/p&gt; &lt;p&gt;With all the information we have so far, run the self-hosted runner with Podman:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman run --secret github_token --name runner -it --rm --privileged -e GH_OWNER='&lt;your github id&gt;' -e GH_REPOSITORY='&lt;repo name&gt;' quay.io/gitaction/runner:2.301.1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If using a different Podman secret name, say &lt;code&gt;some_github_token&lt;/code&gt;, use the extra environment variable &lt;code&gt;GH_TOKEN_PATH&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman run --secret some_github_token --name runner -it --rm --privileged -e GH_OWNER='&lt;your github id&gt;' -e GH_REPOSITORY='&lt;repo name&gt;' -e GH_TOKEN_PATH=/run/secrets/some_github_token quay.io/gitaction/runner:2.301.1&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Pass in extra information&lt;/h3&gt; &lt;p&gt;In reality, E2E CI workflows often require extra information outside of the target GitHub repository.&lt;/p&gt; &lt;p&gt;For illustrative purposes, we use the &lt;a href="https://github.com/redhat-partner-solutions/rhel-sriov-test"&gt;RHEL SR-IOV test suite&lt;/a&gt; as an example throughout this article. Its E2E CI workflow requires testbed information. The testbed information is not checked into the GitHub repository. For the runner container to access this information, a volume mount can be used. You can apply the same technique in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To set up the volume mount for this purpose, first create a folder on the host and copy the required files into this folder. For the RHEL SR-IOV E2E CI, the required files are &lt;code&gt;testbed.yaml&lt;/code&gt; and &lt;code&gt;config.yaml&lt;/code&gt;, so copy these files into the folder and start the container with the volume mount:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo mkdir -p /opt/E2E-config sudo cp testbed.yaml /opt/E2E-config sudo cp config.yaml /opt/E2E-config sudo chown -R nobody:nobody /opt/E2E-config podman run --secret github_token --name runner -it --rm --privileged -e GH_OWNER='redhat-partner-solutions' -e GH_REPOSITORY='rhel-sriov-test' -v /opt/E2E-config:/config quay.io/gitaction/runner:2.301.1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the above sample step, the volume is mounted to &lt;code&gt;/config&lt;/code&gt; inside the container. That means the E2E CI workflow needs to go to this folder to retrieve these YAML files. Interested readers can take a look at the following &lt;a href="https://github.com/redhat-partner-solutions/rhel-sriov-test/blob/main/.github/workflows/e2e.yaml"&gt;E2E CI workflow&lt;/a&gt; for reference.&lt;/p&gt; &lt;h3&gt;Label the runner&lt;/h3&gt; &lt;p&gt;A GitHub repo can have multiple containerized runners on the same server using different labels. This is useful if various tests have different hardware requirements; for example, 800-series and 700-series Intel NICs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman run --secret github_token --name runner810 -it --rm --privileged -e RUNNER_LABEL='810' -e GH_OWNER='redhat-partner-solutions' -e GH_REPOSITORY='rhel-sriov-test' -v /opt/E2E-config-810:/config quay.io/gitaction/runner:2.301.1&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman run --secret github_token --name runner710 -it --rm --privileged -e RUNNER_LABEL='710' -e GH_OWNER='redhat-partner-solutions' -e GH_REPOSITORY='rhel-sriov-test' -v /opt/E2E-config-710:/config quay.io/gitaction/runner:2.301.1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can refer to the runners above as &lt;code&gt;runs-on: [self-hosted, 810]&lt;/code&gt; or &lt;code&gt;runs-on: [self-hosted, 710]&lt;/code&gt;  in a GitHub workflow. &lt;/p&gt; &lt;p&gt;How to use the runner label will be explained later in the &lt;strong&gt;How to trigger the CI&lt;/strong&gt; section.&lt;/p&gt; &lt;h3&gt;Self-hosted runner as a systemd service&lt;/h3&gt; &lt;p&gt;Directly using the Podman command line to start the runner container primarily serves the purpose of proof of concept. For production use, you can use a &lt;a href="https://developers.redhat.com/cheat-sheets/systemd-commands-cheat-sheet"&gt;systemd&lt;/a&gt; service to manage the self-hosted runner. Here is the systemd unit file that was used by the RHEL SR-IOV E2E CI:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[Unit] Description=github self runner in container After=network.target [Service] Type=simple ExecStart=/usr/bin/podman run --secret github_token --name runner --rm --privileged -e GH_OWNER='redhat-partner-solutions' -e GH_REPOSITORY='rhel-sriov-test' -v /opt/E2E-config:/config quay.io/gitaction/runner:2.301.1 ExecStop=/usr/bin/podman stop runner [Install] WantedBy=multi-user.target&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After the container starts and successfully registers with the target GitHub repository, the self-hosted runner can be found under the target repository's Actions/Runners, as shown in Figure 1:&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-embedded"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;div class="field__item"&gt; &lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/runners.png" width="1086" height="582" alt="Runners" typeof="Image" /&gt;&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The self-hosted runner listed in the repository.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;How to trigger the CI&lt;/h2&gt; &lt;p&gt;Here is the sample code for using the runner label:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;name: sriov-e2e-test run-name: sriov-e2e-test initiated by ${{ github.actor }} on: pull_request: types: [ labeled ] workflow_dispatch: inputs: tag: description: 'NIC hardware' required: true default: '810' type: choice options: - 810 - 710 jobs: prepare-label: runs-on: ubuntu-latest outputs: label: ${{ steps.step1.outputs.label }} steps: - name: Check label id: step1 run: | if [ ${{ github.event.label.name }} == 'e2e-test' ]; then echo "label=810" &gt;&gt; $GITHUB_OUTPUT elif [ ${{ github.event.label.name }} == 'e2e-test-710' ]; then echo "label=710" &gt;&gt; $GITHUB_OUTPUT elif [ -n ${{ github.event.inputs.tag }} ]; then echo "label=${{ github.event.inputs.tag }}" &gt;&gt; $GITHUB_OUTPUT fi&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Using a label to trigger an E2E CI action&lt;/h3&gt; &lt;p&gt;As illustrated in the above sample code, one option to trigger the E2E test is to use the appropriate label, &lt;code&gt;e2e-test&lt;/code&gt; or &lt;code&gt;e2e-test-710&lt;/code&gt; (see Figure 2). This labeling mechanism serves as a way to limit who can trigger the E2E runs due to hardware resource constraints. Only repo users with write permission can set a pull request label and trigger the E2E test execution.  &lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-embedded"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;div class="field__item"&gt; &lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/labels.png" width="324" height="104" alt="Labels" typeof="Image" /&gt;&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Triggering a E2E test run with the e2e-test label.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The labels double as a flag showing which PRs have been tested. &lt;/p&gt; &lt;h3&gt;On-demand triggering&lt;/h3&gt; &lt;p&gt;In addition to the labeling above, we can also trigger this E2E action on demand. NIC hardware labels (810 or 710) are collected from the user input, in this case, and used to trigger the appropriate runner.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-embedded"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;div class="field__item"&gt; &lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/e2e_hw_sel_0.png" width="1181" height="522" alt="On demand" typeof="Image" /&gt;&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Self-hosted runner as an OpenShift Workload&lt;/h2&gt; &lt;p&gt;If the test environment already has an OpenShift/&lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster installed, and the user does not plan to add an extra RHEL server to host the runner systemd service, the runner container can be hosted on the OpenShift/Kubernetes cluster instead. In this situation, the self-hosted runner will be a workload in the pod format.&lt;/p&gt; &lt;p&gt;To use the runner container as an OpenShift workload for controlling an on-premise E2E CI testbed, the OpenShift cluster needs to be on-premise and have connectivity to the E2E CI testbed.&lt;/p&gt; &lt;p&gt;We will need to take steps to protect the user's PAT and pass in extra test configuration, similar to the runner container.&lt;/p&gt; &lt;h3&gt;GitHub token protection&lt;/h3&gt; &lt;p&gt;In OpenShift, create a secret for the PAT:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl create secret generic gh-token --from-literal=github_token=&lt;your github token&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the above command, the name &lt;code&gt;github_token&lt;/code&gt; is used for the same reason explained earlier in the podman usage.&lt;/p&gt; &lt;p&gt;The secret &lt;code&gt;gh-token&lt;/code&gt; will be mounted as a volume later in the runner pod YAML spec.&lt;/p&gt; &lt;h3&gt;Pass in extra information&lt;/h3&gt; &lt;p&gt;Once again using the &lt;a href="https://github.com/redhat-partner-solutions/rhel-sriov-test"&gt;RHEL SR-IOV test suite repository&lt;/a&gt; for demo purposes, its E2E CI workflow requires &lt;code&gt;testbed.yaml&lt;/code&gt; and &lt;code&gt;config.yaml&lt;/code&gt; files, which can be passed to the runner pod via a volume map.&lt;/p&gt; &lt;p&gt;First, create a folder and store the required files under this folder:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ls /opt/E2E-config config.yaml testbed.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a ConfigMap from this folder:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc create configmap test-config --from-file=/opt/E2E-config&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This ConfigMap, &lt;code&gt;test-config&lt;/code&gt;, will be used in the volume map of the runner pod YAML spec.&lt;/p&gt; &lt;h3&gt;Self-hosted runner in deployment&lt;/h3&gt; &lt;p&gt;We will let OpenShift take care of the runner pod lifecycle management using a deployment. Here is the self-hosted runner deployment YAML spec:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: apps/v1 kind: Deployment metadata: name: runner-deployment labels: app: runner spec: replicas: 1 selector: matchLabels: app: runner template: metadata: labels: app: runner spec: volumes: - name: secret-volume secret: secretName: gh-token - name: config-volume configMap: name: test-config containers: - name: runner image: quay.io/gitaction/runner:2.301.1 securityContext: privileged: true env: - name: GH_OWNER value: "redhat-partner-solutions" - name: GH_REPOSITORY value: "rhel-sriov-test" - name: GH_TOKEN_PATH value: "/etc/gh_secrets/github_token" volumeMounts: - name: secret-volume readOnly: true mountPath: "/etc/gh_secrets" - name: config-volume mountPath: "/config"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice in the above YAML spec the OpenShift secret &lt;code&gt;gh-token&lt;/code&gt; is mounted to the path &lt;code&gt;/etc/gh_secrets&lt;/code&gt;, so the environment variable &lt;code&gt;GH_TOKEN_PATH&lt;/code&gt; is used to tell the container to retrieve the secret from this path.&lt;/p&gt; &lt;p&gt;The extra information for the E2E CI testbed is mounted under &lt;code&gt;/config&lt;/code&gt;. As explained earlier, the E2E workflow will look for the extra information in that folder inside the container.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;We reviewed E2E CI building blocks which allow real hardware test execution for a GitHub open source project. Lightweight GitHub Actions CI, along with the containerized GitHub Actions runner, allow minimizing system footprint while maintaining CI functionality. Furthermore, this CI implementation fits well into corporate IT security policy for lab access: nothing extra gets exposed to the internet.&lt;/p&gt; &lt;p&gt;Feel free to comment below if you have questions. We welcome your feedback!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/25/end-end-testing-self-hosted-runners-github-actions" title="End-to-end testing with self-hosted runners in GitHub Actions"&gt;End-to-end testing with self-hosted runners in GitHub Actions&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jianzhu Zhang, Andrew Kiselev, Daniel Kostecki</dc:creator><dc:date>2023-07-25T07:00:00Z</dc:date></entry><entry><title type="html">How to upgrade WildFly JSF version with Galleon</title><link rel="alternate" href="https://www.mastertheboss.com/java-ee/jsf/how-to-upgrade-wildfly-jsf-version-with-galleon/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/java-ee/jsf/how-to-upgrade-wildfly-jsf-version-with-galleon/</id><updated>2023-07-24T08:10:31Z</updated><content type="html">This article will teach you how to add MyFaces 4 (or newer) support to your WildFly installation using Galleon features pack. At the end of it, you will be able to complete the upgrade of your JSF implementation in no time! Choosing a different JSF Implementation in WildFly Out of the box, WildFly ships with ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to integrate Spring Boot 3, Spring Security, and Keycloak</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/24/how-integrate-spring-boot-3-spring-security-and-keycloak" /><author><name>Muhammad Edwin</name></author><id>16141334-921b-453f-8708-44ee5d9057f6</id><updated>2023-07-24T07:00:00Z</updated><published>2023-07-24T07:00:00Z</published><summary type="html">&lt;p&gt;Quite some time ago, Keycloak &lt;a href="https://www.keycloak.org/2022/02/adapter-deprecation.html"&gt;deprecated its adapters&lt;/a&gt;, including OpenID connect for &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; adapters. For &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt; developers, this means we need to use Spring Security for OpenID and OAuth2 connectivity with Keycloak instead of relying on Keycloak adapters.&lt;/p&gt; &lt;p&gt;In this article, we'll create a sample Java application on top of Spring Boot 3 and protect it by using Spring Security and Keycloak, without having to use Keycloak adapters.&lt;/p&gt; &lt;h2&gt;Install Keycloak&lt;/h2&gt; &lt;p&gt;First, we need to install Keycloak to our system. In this example, we are using Keycloak 17 and installing it using a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt;. Here we've used &lt;code&gt;admin&lt;/code&gt; as the administrator username and &lt;code&gt;password&lt;/code&gt; as its password.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker pull keycloak/keycloak:17.0.0 $ docker run -p 8080:8080 \ -e KEYCLOAK_ADMIN=admin \ -e KEYCLOAK_ADMIN_PASSWORD=password \ keycloak/keycloak:17.0.0 start-dev &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can open the login page and input our credentials there (Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boot1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/boot1.png?itok=n2KVLgRX" width="600" height="331" alt="Keycloak login page" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Keycloak login page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After login, we can create a new "realm" with the name &lt;code&gt;External&lt;/code&gt; (Figure 2).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boot2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/boot2.png?itok=oOOz-K1q" width="600" height="187" alt="add new keycloak realm" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Creating a new external realm in Keycloak.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once we have our realm, let's start creating Keycloak clients with the name of &lt;code&gt;external-client&lt;/code&gt; (Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boot3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/boot3.png?itok=Z-otUKP1" width="600" height="214" alt="add client" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Setting up the Keycloak client.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Make sure to configure the client as follows:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Client ID: &lt;code&gt;external-client&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Enabled: On&lt;/li&gt; &lt;li&gt;Client Protocol: &lt;code&gt;openid-connect&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Access type: Confidential&lt;/li&gt; &lt;li&gt;Standard flow enabled: On&lt;/li&gt; &lt;li&gt;Direct access grants enabled: On&lt;/li&gt; &lt;li&gt;Valid redirects URI: &lt;code&gt;http://localhost:8081/*&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Capture the client secret, as shown in Figure 4.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boot4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/boot4.png?itok=Bx1gotbx" width="600" height="212" alt="add keycloak client secret" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The client secret.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next, create a new user for this Realm (Figure 5).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boot5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/boot5.png?itok=jDw-dTkk" width="600" height="418" alt="add keycloak user" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Adding a sample user.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After that, we can create a password for this user (Figure 6).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boot6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/boot6.png?itok=Fpgwa2AH" width="600" height="316" alt="password for keycloak user" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Setting the password for the new user.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once you have completed all of the preceding steps, you are ready to proceed to the next section.&lt;/p&gt; &lt;h2&gt;Spring Boot 3&lt;/h2&gt; &lt;p&gt;First, we need to define the Spring version in our &lt;code&gt;pom.xml&lt;/code&gt; file. For this sample we are using Spring 3.0.4 and Java 17.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.edw&lt;/groupId&gt; &lt;artifactId&gt;spring-3-keycloak&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;3.0.4&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;17&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;17&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-oauth2-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Define the Keycloak integration&lt;/h2&gt; &lt;p&gt;We can define our Keycloak integration by setting them in our &lt;code&gt;application.properties&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;### server port server.port=8081 spring.application.name=Spring 3 and Keycloak ## logging logging.level.org.springframework.security=INFO logging.pattern.console=%d{dd-MM-yyyy HH:mm:ss} %magenta([%thread]) %highlight(%-5level) %logger.%M - %msg%n ## keycloak spring.security.oauth2.client.provider.external.issuer-uri=http://localhost:8080/realms/external spring.security.oauth2.client.registration.external.provider=external spring.security.oauth2.client.registration.external.client-name=external-client spring.security.oauth2.client.registration.external.client-id=external-client spring.security.oauth2.client.registration.external.client-secret=(put your client secret here) spring.security.oauth2.client.registration.external.scope=openid,offline_access,profile spring.security.oauth2.client.registration.external.authorization-grant-type=authorization_code &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the Java files&lt;/h2&gt; &lt;p&gt;Once we define our configuration, the next step is to create our Java files. We can start with our security configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.security.config.annotation.web.builders.HttpSecurity; import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity; import org.springframework.security.config.http.SessionCreationPolicy; import org.springframework.security.web.SecurityFilterChain; @Configuration @EnableWebSecurity public class SecurityConfiguration { @Bean public SecurityFilterChain configure(HttpSecurity http) throws Exception { http .oauth2Client() .and() .oauth2Login() .tokenEndpoint() .and() .userInfoEndpoint(); http .sessionManagement() .sessionCreationPolicy(SessionCreationPolicy.ALWAYS); http .authorizeHttpRequests() .requestMatchers("/unauthenticated", "/oauth2/**", "/login/**").permitAll() .anyRequest() .fullyAuthenticated() .and() .logout() .logoutSuccessUrl("http://localhost:8080/realms/external/protocol/openid-connect/logout?redirect_uri=http://localhost:8081/"); return http.build(); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we'll create our controller and main Java files:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.edw.controller; import org.springframework.security.core.context.SecurityContextHolder; import org.springframework.security.oauth2.core.user.OAuth2User; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import java.util.HashMap; @RestController public class IndexController { @GetMapping(path = "/") public HashMap index() { // get a successful user login OAuth2User user = ((OAuth2User)SecurityContextHolder.getContext().getAuthentication().getPrincipal()); return new HashMap(){{ put("hello", user.getAttribute("name")); put("your email is", user.getAttribute("email")); }}; } @GetMapping(path = "/unauthenticated") public HashMap unauthenticatedRequests() { return new HashMap(){{ put("this is ", "unauthenticated endpoint"); }}; } } &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package com.edw; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class Main { public static void main(String[] args) { SpringApplication.run(Main.class, args); } } &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Test the application&lt;/h2&gt; &lt;p&gt;We can directly open our Java application's URL located in port 8081 and be automatically redirected to our Keycloak login page. We can also check using a cURL command to see what is happening behind the scenes:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ curl -v http://localhost:8081/ * Trying ::1:8081... * TCP_NODELAY set * Connected to localhost (::1) port 8081 (#0) &gt; GET / HTTP/1.1 &gt; Host: localhost:8081 &gt; User-Agent: curl/7.65.0 &gt; Accept: */* &gt; * Mark bundle as not supporting multiuse &lt; HTTP/1.1 302 &lt; Set-Cookie: JSESSIONID=D002DC6523769DB2D4D0559D851575E6; Path=/; HttpOnly &lt; X-Content-Type-Options: nosniff &lt; X-XSS-Protection: 0 &lt; Cache-Control: no-cache, no-store, max-age=0, must-revalidate &lt; Pragma: no-cache &lt; Expires: 0 &lt; X-Frame-Options: DENY &lt; Location: http://localhost:8081/oauth2/authorization/external &lt; Content-Length: 0 &lt; Date: Mon, 27 Mar 2023 07:08:27 GMT &lt; * Connection #0 to host localhost left intact $ curl -v http://localhost:8081/oauth2/authorization/external * Trying ::1:8081... * TCP_NODELAY set * Connected to localhost (::1) port 8081 (#0) &gt; GET /oauth2/authorization/external HTTP/1.1 &gt; Host: localhost:8081 &gt; User-Agent: curl/7.65.0 &gt; Accept: */* &gt; * Mark bundle as not supporting multiuse &lt; HTTP/1.1 302 &lt; Set-Cookie: JSESSIONID=73BF322BC83966BF49C39398ACD20DAB; Path=/; HttpOnly &lt; X-Content-Type-Options: nosniff &lt; X-XSS-Protection: 0 &lt; Cache-Control: no-cache, no-store, max-age=0, must-revalidate &lt; Pragma: no-cache &lt; Expires: 0 &lt; X-Frame-Options: DENY &lt; Location: http://localhost:8080/realms/external/protocol/openid-connect/auth?response_type=code&amp;client_id=external-client&amp;scope=openid%20offline_access%20profile&amp;state=5wK6GouLBPi3DU1hu_AqcoDHefWNt67G5sPfGxfjZtk%3D&amp;redirect_uri=http://localhost:8081/login/oauth2/code/external&amp;nonce=5A8TcFCXueHsf2xBXJQ_NXEjmOtK4BwRh4uvI-kvvIs &lt; Content-Length: 0 &lt; Date: Mon, 27 Mar 2023 07:08:58 GMT &lt; * Connection #0 to host localhost left intact &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can see that all requests to the root URL have a 302 HTTP response, indicating that our application is protected by the Keycloak login page. However, we can test our whitelist insecure URL and see that we can access it directly without having to log in first.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ curl -v http://localhost:8081/unauthenticated * Trying ::1:8081... * TCP_NODELAY set * Connected to localhost (::1) port 8081 (#0) &gt; GET /unauthenticated HTTP/1.1 &gt; Host: localhost:8081 &gt; User-Agent: curl/7.65.0 &gt; Accept: */* &gt; * Mark bundle as not supporting multiuse &lt; HTTP/1.1 200 &lt; Set-Cookie: JSESSIONID=22CA2E6EE6B79F7FD649592D87405C71; Path=/; HttpOnly &lt; X-Content-Type-Options: nosniff &lt; X-XSS-Protection: 0 &lt; Cache-Control: no-cache, no-store, max-age=0, must-revalidate &lt; Pragma: no-cache &lt; Expires: 0 &lt; X-Frame-Options: DENY &lt; Content-Type: application/json &lt; Transfer-Encoding: chunked &lt; Date: Mon, 27 Mar 2023 07:13:00 GMT &lt; * Connection #0 to host localhost left intact {"this is ": "unauthenticated endpoint"} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's try to insert our username and password into the login page. We can see the result is there, as shown in Figure 7.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boot8.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/boot8.png?itok=4jL10vsg" width="600" height="444" alt="insert username and password to keycloak login page" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: The Keycloak sign-in page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We can see the result after login, as shown in Figure 8.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boot9.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/boot9.png?itok=mj9Nb2uu" width="600" height="189" alt="rest api result after login with keycloak credentials" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: The login is successful&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This article showed how Spring Boot 3 and Spring Security can connect to Keycloak using the default Oauth2 client library that comes with Spring Boot (spring-boot-starter-oauth2-client).&lt;/p&gt; &lt;p&gt;Code for this project can be accessed at &lt;a href="https://github.com/edwin/spring-3-keycloak"&gt;https://github.com/edwin/spring-3-keycloak&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/24/how-integrate-spring-boot-3-spring-security-and-keycloak" title="How to integrate Spring Boot 3, Spring Security, and Keycloak"&gt;How to integrate Spring Boot 3, Spring Security, and Keycloak&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Muhammad Edwin</dc:creator><dc:date>2023-07-24T07:00:00Z</dc:date></entry><entry><title>Use JFR to profile and monitor native executables</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/profile-and-monitor-native-executables-with-jfr/&#xA;            " /><author><name>Robert Toyonaga</name></author><id>https://quarkus.io/blog/profile-and-monitor-native-executables-with-jfr/</id><updated>2023-07-24T00:00:00Z</updated><published>2023-07-24T00:00:00Z</published><summary type="html">Quarkus native executables offer many benefits such as even faster start-up time and low footprint. However, one major drawback is that such native binaries can be less observable. The good news is that you can still use JDK Flight Recorder (JFR) when running your Quarkus applications as native executables. Native...</summary><dc:creator>Robert Toyonaga</dc:creator><dc:date>2023-07-24T00:00:00Z</dc:date></entry><entry><title type="html">WildFly 29 is released!</title><link rel="alternate" href="https://wildfly.org//news/2023/07/21/WildFly29-Released/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2023/07/21/WildFly29-Released/</id><updated>2023-07-21T00:00:00Z</updated><content type="html">I’m pleased to announce that the new WildFly and WildFly Preview 29.0.0.Final releases are available for download at . NEW AND NOTABLE During the WildFly 29 development cycle the WildFly contributors were heavily focused on bug fixing, plus a lot internal housekeeping that needed doing after all the recent work toward Jakarta EE 10. But we do have some new goodies: * It is now possible to . * You can use Galleon to using the new Keycloak SAML Adapter feature pack. * You can use Galleon to using the 1.0.0.Beta1 release of the new . (Note that the feature pack is still a Beta.) * The elytron subsystem’s new Distributed Realm attribute ignore-unavailable-realms enables a user to . SUPPORTED SPECIFICATIONS JAKARTA EE WildFly 29 is a compatible implementation of the EE 10 as well as the and the . WildFly is EE 10 compatible when running on both Java SE 11 and Java SE 17. Evidence supporting our certification is available in the repository on GitHub: Specification Compatibility Evidence Jakarta EE 10 Full Platform Jakarta EE 10 Web Profile Jakarta EE 10 Core Profile MICROPROFILE WildFly supports numerous MicroProfile specifications. Because we no longer support MicroProfile Metrics, WildFly 28 cannot claim to be a compatible implementation of the MicroProfile 6.0 specification. However, WildFly’s MicroProfile support includes implementations of the following specifications in our "full" (e.g. standalone-full.xml) and "default" (e.g standalone.xml) configurations as well as our "microprofile" configurations (e.g. standalone-microprofile.xml): MicroProfile Technology WildFly Full/Default Configurations WildFly MicroProfile Configuration MicroProfile Config 3.0 X X MicroProfile Fault Tolerance 4.0  —  X MicroProfile Health 4.0  —  X MicroProfile JWT Authentication 2.1 X X MicroProfile LRA 2.0  —  X MicroProfile OpenAPI 3.1  —  X MicroProfile Open Telemetry 1.0  —  X MicroProfile Reactive Messaging 3.0  —   —  MicroProfile Reactive Streams Operators 3.0  —   —  MicroProfile Rest Client 3.0 X X for the above specifications that are part of MicroProfile 6.0 can be found in the WildFly Certifications repository on GitHub. JAVA SE SUPPORT Our recommendation is that you run WildFly on the most recent long-term support Java SE release, i.e. on SE 17 for WildFly 29. While we do do some testing of WildFly on JDK 20, we do considerably more testing of WildFly itself on the LTS JDKs, and we make no attempt to ensure the projects producing the various libraries we integrate are testing their libraries on anything other than JDK 11 or 17. WildFly 29 also is heavily tested and runs well on Java 11. We anticipate continuing to support Java 11 at least through WildFly 30, and perhaps beyond. We do, however, anticipate removing support for SE 11 sometime in the next 12 to 18 months. While we recommend using an LTS JDK release, I do believe WildFly runs well on JDK 20. By runs well, I mean the main WildFly testsuite runs with no more than a few failures in areas not expected to be commonly used. We want developers who are trying to evaluate what a newer JVM means for their applications to be able to look to WildFly as a useful development platform. Please note that WildFly runs in classpath mode. MIGRATION ISSUES In this section of these release announcements I’ll note issues users may experience when migrating from a previous version of WildFly. GALLEON TOOLING We’ve added additional metadata to the files provide to the Galleon tooling, with the aim of providing exciting new provisioning capabilities. (Keep an eye out for more on those new capabilities in a future WildFly release.) This addition, however necessitates that in order to provision WildFly 29 users must update the versions of Galleon-related tooling they use. * For those who use the to provision WildFly, or later is required. * or later is also required for users of the * For those who use the to provision WildFly, or later is required. * For users who use the to produce a bootable jar: or later is required. We are working to add improved forward compatibility to our tooling to help reduce the likelihood that future improvements will require users to update their tooling versions in order to work with newer releases of WildFly. RELEASE NOTES The full release notes for the release are in the . Issues fixed in the underlying release are listed in the WildFly Core JIRA. Please try it out and give us your feedback, in the , or . Meanwhile, we’re busy at work on WildFly 30! Best regards, Brian</content><dc:creator>Brian Stansberry</dc:creator></entry><entry><title>How to balance per-CPU upcall dispatch mode in Open vSwitch</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/20/how-balance-cpu-upcall-dispatch-mode-open-vswitch" /><author><name>Michael Santana</name></author><id>8cdce30c-414d-4d73-b63f-953d4924da75</id><updated>2023-07-20T07:00:00Z</updated><published>2023-07-20T07:00:00Z</published><summary type="html">&lt;p&gt;Open vSwitch has moved away from using per-vport dispatch mode to using per-CPU dispatch mode. But this mode had issues with upcall handler thread imbalance and CPU mismatch error messages. These issues were mostly found in systems with tuned CPUs.&lt;/p&gt; &lt;p&gt;This article explains two main fixes that my patch series applied to Open vSwitch that alleviated these issues. The first fix resulted in the ovs-vswitchd sending an array of a size that the Open vSwitch kernel module will accept and not trigger the CPU mismatch error message. The second fix added additional upcall handler threads in cases of tuned CPUs to create a more balanced workload for the upcall handler threads.&lt;/p&gt; &lt;h2&gt;History of dispatch modes in Open vSwitch&lt;/h2&gt; &lt;p&gt;In July 2021, my former colleague, Mark Grey, &lt;a href="https://github.com/openvswitch/ovs/commit/b1e517bd2f818fc7c0cd43ee0b67db4274e6b972" target="_blank"&gt;introduced per-CPU dispatch mode to Open vSwitch&lt;/a&gt;. This dispatch mode was made to fix issues found in the old &lt;code&gt;per-vport&lt;/code&gt; dispatch mode. In Open vSwitch, the &lt;code&gt;per-vport&lt;/code&gt; dispatch mode creates a netlink socket for each vport. The introduction of &lt;code&gt;per-CPU&lt;/code&gt; dispatch mode fixed a number of issues found in &lt;code&gt;per-vport&lt;/code&gt; mode including &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1844576" target="_blank"&gt;packet reordering&lt;/a&gt; and &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1834444" target="_blank"&gt;thundering herd&lt;/a&gt; issues.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;per-CPU&lt;/code&gt; dispatch mode differs from &lt;code&gt;per-vport&lt;/code&gt; mode in that the number of netlink sockets created in the &lt;code&gt;per-CPU&lt;/code&gt; dispatch mode is equal to the number of upcall handler threads created. And most importantly, each netlink socket maps to exactly one upcall handler thread. The idea behind &lt;code&gt;per-CPU&lt;/code&gt; dispatch mode is to have a one-to-one correspondence between CPU, netlink socket, and upcall handler thread, as shown in the diagram in Figure 1. This idea fixed the old issues in &lt;code&gt;per-vport&lt;/code&gt; mode.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/handlers_one_to_one_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/handlers_one_to_one_0.png?itok=-_NQtHmx" width="600" height="257" alt="A diagram showing handlers one-to-one mapping between the CPU, netlink socket, and handler thread." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The handlers one-to-one mapping between the CPU, netlink socket, and handler thread.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;A user can determine which mode their system is running via the following command: &lt;code&gt;ovs-appctl dpif-netlink/dispatch-mode&lt;/code&gt;. The dispatch mode defaults to per-CPU when using a kernel &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1834444#c12" target="_blank"&gt;at least &lt;/a&gt;&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1834444#c12" target="_blank"&gt;&lt;code&gt;4.18.0-328.el8.mr1064_210805_1629&lt;/code&gt;&lt;/a&gt; and cannot be changed by the user.&lt;/p&gt; &lt;p&gt;One intentional side effect of the &lt;code&gt;per-CPU&lt;/code&gt; dispatch mode is that now users &lt;strong&gt;cannot&lt;/strong&gt; change the number of upcall handler threads via the user-configurable variable &lt;code&gt;n-handler-threads&lt;/code&gt; in &lt;code&gt;per-CPU&lt;/code&gt; dispatch mode. The user-configurable variable &lt;code&gt;n-handler-threads&lt;/code&gt; only works in &lt;code&gt;per-vport&lt;/code&gt; dispatch mode.&lt;code&gt; &lt;/code&gt;In &lt;code&gt;per-CPU&lt;/code&gt; dispatch mode, the number of upcall handler threads is determined by the number CPUs in which &lt;code&gt;ovs-vswitchd&lt;/code&gt; can run on, which is largely affected by the affinity of &lt;code&gt;ovs-vswitchd&lt;/code&gt; or the number of tuned CPUs via CPU isolation.&lt;/p&gt; &lt;p&gt;After merging the &lt;code&gt;per-CPU&lt;/code&gt; dispatch mode, there were bugs (&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=2102449" target="_blank"&gt;BZ#2102449&lt;/a&gt;, &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=2006605" target="_blank"&gt;BZ#2006605&lt;/a&gt;), reporting that Open vSwitch kernel module threw the following error message: &lt;code&gt;openvswitch: cpu_id mismatch with handler threads&lt;/code&gt;. These CPU mismatch error messages typically happened on systems with tuned CPUs or disabled cores.&lt;/p&gt; &lt;h2&gt;What caused the problem?&lt;/h2&gt; &lt;p&gt;The CPU mismatch error message originates from the fact that the Openvswitch Kernel Module expects an array with a size equal to the number of CPUs in the system as shown in the following code snippet in the Linux kernel &lt;code&gt;net/openvswitch/datapath.c&lt;/code&gt; where &lt;code&gt;handlers_array_size&lt;/code&gt; is the size of &lt;code&gt;handlers_array&lt;/code&gt; and &lt;code&gt;cpu_id&lt;/code&gt; is the CPU in which the upcall happened on the system.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-c"&gt;if (handlers_array_size &gt; 0 &amp;&amp; cpu_id &gt;= handlers_array_size) { pr_info_ratelimited("cpu_id mismatch with handler threads"); return handlers_array[cpu_id % handlers_array_size]; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Each index of the &lt;code&gt;handlers_array&lt;/code&gt; corresponds to the CPU ID on which an upcall can occur and each element in the array is a netlink socket file descriptor corresponding to an upcall handler thread. The array maps upcalls that happen on a particular CPU to a corresponding upcall handler thread via the contained netlink socket file descriptor in the array. The array determines which upcall handler thread will receive the upcall.&lt;/p&gt; &lt;p&gt;The problem is that &lt;code&gt;ovs-vswitchd&lt;/code&gt; was not sending an array of the expected size, and it was not sending a mapping for all CPUs. Instead, it was sending an array with a size equal to the number of upcall handler threads. As mentioned earlier, the default behavior in &lt;code&gt;ovs-vswitchd&lt;/code&gt; is to create as many upcall handler threads as there are available CPUs for &lt;code&gt;ovs-vswitchd&lt;/code&gt; to run on. This is not a problem when the CPU affinity of &lt;code&gt;ovs-vswitchd&lt;/code&gt; is not set.&lt;/p&gt; &lt;p&gt;However, it is a problem when the CPUs are tuned or disabled, or the affinity of &lt;code&gt;ovs-vswitchd&lt;/code&gt; is changed. The default behavior is to create as many upcall handler threads as there are available CPUs for &lt;code&gt;ovs-vswitchd&lt;/code&gt; to run on, but if we have tuned or disabled CPUs the number of upcall handler threads will be &lt;em&gt;l&lt;/em&gt;ess than the total number of CPUs. We would hit the condition &lt;code&gt;cpu_id &gt;= handlers_array_size&lt;/code&gt; when the system receives an upcall on a CPU ID that is larger or equal than &lt;code&gt;handlers_array_size&lt;/code&gt; causing the CPU mismatch error message (Figure 2).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/handlers_without_fix.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/handlers_without_fix.png?itok=Z998Pizw" width="600" height="251" alt="A diagram of the handlers without the fix." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The handlers without the fix is in imbalance.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;The CPU mismatch impact&lt;/h2&gt; &lt;p&gt;The most visible impact is the CPU mismatch error message. But there are other more severe issues that can occur. One of them is overloading upcall handler threads while simultaneously starving others in the case of fixed Receive Packet Steering (RPS), powered off CPUs, or unplugged CPUs, which will be discussed later. This can happen as a side effect of the Open vSwitch kernel module using a modulo operation (shown in the previous code snippet) on the array when there is a CPU mismatch. In essence, some upcall handler threads get all the workload, while others are always on standby. This is an obvious misuse of CPU utilization.&lt;/p&gt; &lt;p&gt;There is also an issue of not having enough upcall handler threads to service the upcalls. Isolated CPU cores can still receive packets even if the core is isolated, and can still trigger an upcall. We still need upcall handler threads to service upcalls triggered by isolated CPUs. We cannot just create as many upcall handler threads as there are non-isolated CPUs because we would underperform in cases with high upcall usage across non-isolated and isolated CPUs. So we should ideally increase the number of upcall handler threads in CPU isolation cases to create a more balanced workload.&lt;/p&gt; &lt;p&gt;Figure 2 shows a system configured with four active CPUs for &lt;code&gt;ovs-vswitchd&lt;/code&gt; to use, but it only created four upcall handler threads. Two of these upcall handler threads (H0, H1) have to service three CPUs each while the other two upcall handler threads (H2, H3) only have to service two CPUs.&lt;/p&gt; &lt;h2&gt;2 Solutions to improve CPU performance&lt;/h2&gt; &lt;p&gt;The fix is two-fold. First, create an array that’s big enough to not trigger the CPU mismatch error message. How we go about deciding how big to create the array is not as straightforward as one would think. Count the total number of CPUs regardless of if they are active or not. That would work, but not on all systems. Some systems have noncontinuous CPU Core IDs. For example, the largest CPU core ID in a particular system is CPU9, but this system only has four CPUs (Figure 3).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/handlers_with_fix_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/handlers_with_fix_0.png?itok=6_eiBY0z" width="600" height="257" alt="A diagram of Open vSwitch handlers with the fix." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The handlers with the fix.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If &lt;code&gt;ovs-vswitchd&lt;/code&gt; sent an array of size 4 to the Open vSwitch kernel module and the system gets an upcall on CPU9, it would still throw a CPU mismatch error message, as previously explained.&lt;/p&gt; &lt;p&gt;Instead, &lt;code&gt;ovs-vswitchd&lt;/code&gt; sends the OVS kernel module an array of size:&lt;/p&gt; &lt;p&gt;&lt;code&gt;size = MAX(count_total_cores(), largest_cpu_id + 1)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The &lt;code&gt;count_total_cores&lt;/code&gt; would be the total number of CPUs, and &lt;code&gt;largest_cpu_id&lt;/code&gt; is the largest CPU in the system. In the previous example, this would be size 4 and CPU9 (don’t forget to add +1) respectively, then pick the largest of the two numbers. This guarantees that the &lt;code&gt;ovs-vswitchd&lt;/code&gt; sends the size the Open vSwitch kernel module expects. This prevents the CPU mismatch error message. The reason this works is because the Openvswitch Kernel Module side gets the upcall from CPU9 and checks to see if CPU9 is less than the array size 10 the &lt;code&gt;ovs-vswitchd&lt;/code&gt; sent. Since 9 is less than 10, we do not trigger the mismatch. If &lt;code&gt;ovs-vswitchd&lt;/code&gt; had sent an array of size 4, we would have triggered a CPU mismatch. This is the complete fix.&lt;/p&gt; &lt;p&gt;Secondly, we decided to go one step forward and try to create a more fair distribution of upcalls amongst the upcall handler threads by increasing the number of upcall handler threads. An upcall can happen on any CPU, even on CPUs that are isolated. Normally, &lt;code&gt;ovs-vswitchd&lt;/code&gt; does not have isolated CPUs. In this case, &lt;code&gt;ovs-vswitchd&lt;/code&gt; creates as many upcall handler threads as there are CPUs.&lt;/p&gt; &lt;p&gt;But in the case that we have isolated CPUs &lt;code&gt;ovs-vswitchd&lt;/code&gt; would normally create as many upcall handler threads as there are non-isolated CPUs. This can be an issue because &lt;code&gt;ovs-vswitchd&lt;/code&gt; still need upcall handler threads to service the upcalls that happen on isolated CPUs.&lt;/p&gt; &lt;p&gt;This is where the additional upcall handler threads come in. Having additional upcall handler threads provides the system with a more fair distribution of upcall workload amongst isolated and non-isolated CPUs. This also improves the imbalance of overloading some upcall handler threads that are found in fixed Receive Packet Steering (RPS), powered off CPUs, or unplugged CPUs. But it does not completely fix it. It helps because the more threads, the more the likely that each actually active CPU gets a unique upcall handler thread and reduces the amount of workload per thread.&lt;/p&gt; &lt;p&gt;The formula for deciding the number of upcall handler threads is as follows:&lt;/p&gt; &lt;p&gt;&lt;code&gt;handlers_n = min(next_prime(active_cores+1), total_cores)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Where &lt;code&gt;next_prime(argument)&lt;/code&gt; is a function that returns the &lt;code&gt;argument&lt;/code&gt; if &lt;code&gt;argument&lt;/code&gt; is a prime number or the next prime after &lt;code&gt;argument&lt;/code&gt;. This guarantees that &lt;code&gt;ovs-vswitchd&lt;/code&gt; has additional upcall handler threads in the case that the system has CPU isolation, but not exceed the maximum number of CPUs. The &lt;code&gt;ovs-vswitchd&lt;/code&gt; fills in the array in a round-robin fashion where &lt;code&gt;ovs-vswitchd&lt;/code&gt; rotates the upcall handler threads. &lt;code&gt;ovs-vswitchd&lt;/code&gt; also uses the modulo operator to restart counting from the first upcall handler thread.&lt;/p&gt; &lt;p&gt;I took all the above suggestions and got &lt;a href="https://github.com/openvswitch/ovs/commit/2803b3fb53f11cc6398294b9b6c365f53d50e0d3"&gt;a patch series merged into Open vSwitch upstream that fixes the Upcall Handler Thread mapping&lt;/a&gt;. In Figure 3, we have a system with my patch series applied with ten CPUs, but only four of them are actually available for &lt;code&gt;ovs-vswitchd&lt;/code&gt; to use. In this example, &lt;code&gt;ovs-vswitchd&lt;/code&gt; creates five upcall handler threads based on the previous equation when using &lt;code&gt;active_cores = 4, total_cores = 10&lt;/code&gt;. Also, each upcall handler threads get the same number of CPUs they need to service.&lt;/p&gt; &lt;h2&gt;Caveats&lt;/h2&gt; &lt;p&gt;This implementation is not a perfect solution. There are many edge cases that this implementation does not address. But we have found that this implementation is the most balanced solution. A better implementation might dynamically change which handlers serve which CPUs according to the RPS scheme used by the NIC for the most optimized performance, but that comes at additional computational overhead.&lt;/p&gt; &lt;p&gt;This implementation is not aware of which CPUs are non-existent or plugged in, which means that there could be cases where you could overload handlers while starving others. This is true in cases of non-existing CPUs or hot-swappable CPUs.&lt;/p&gt; &lt;h2&gt;Related works&lt;/h2&gt; &lt;p&gt;My colleague, Adrián Moreno Zapata, built on top of my work and got a &lt;a href="https://github.com/openvswitch/ovs/commit/0d23948a598ac609e9865174e0874e782a48d6a8" target="_blank"&gt;patch series merged&lt;/a&gt; that allows for a dynamic number of upcall handler threads when the number of CPUs change during ovs-vswitchd run time. The number of CPUs can change for many reasons during run time, including CPUs being hot-plugged, switched on or off, or affinity mask of ovs-vswitchd changing. Another colleague, &lt;a href="https://developers.redhat.com/author/echaudron"&gt;Eelco Chaudron&lt;/a&gt; took a deep dive into the world of &lt;a href="https://developers.redhat.com/articles/2022/10/19/open-vswitch-revalidator-process-explained" target="_blank"&gt;revalidator threads&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/20/how-balance-cpu-upcall-dispatch-mode-open-vswitch" title="How to balance per-CPU upcall dispatch mode in Open vSwitch"&gt;How to balance per-CPU upcall dispatch mode in Open vSwitch&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Santana</dc:creator><dc:date>2023-07-20T07:00:00Z</dc:date></entry><entry><title>How to retrieve packet drop reasons in the Linux kernel</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/19/how-retrieve-packet-drop-reasons-linux-kernel" /><author><name>Antoine Tenart</name></author><id>c81250ec-0fca-472f-9d40-943ba5153745</id><updated>2023-07-19T07:00:00Z</updated><published>2023-07-19T07:00:00Z</published><summary type="html">&lt;p&gt;Understanding why a packet drops in the Linux kernel is not always easy. The networking stack is wide and reasons to refuse a given packet are multiple and include invalid data from a protocol, firewall rules, wrong checksum, full queues, qdisc or XDP actions, and many more reasons. It is possible to look at indicators such as MIB counters and statistic counters, but often those are generic and triggered for different reasons, but most importantly their coverage is small, and it's impossible to match a specific packet to a given counter increase. &lt;/p&gt; &lt;h2&gt;Socket buffer drop reasons&lt;/h2&gt; &lt;p&gt;The socket buffer, SKB (&lt;code&gt;struct sk_buff&lt;/code&gt;) is the main data structure representing a packet in the Linux kernel networking stack. When a packet is dropped in the Linux kernel, in most cases, it means its associated socket buffer has dropped. In recent versions of the Linux kernel, starting in v5.17, socket buffers can be dropped with an associated reason. This was introduced in upstream commit &lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=c504e5c2f9648a1e5c2be01e8c3f59d394192bd3"&gt;c504e5c2f964 ("net: skb: introduce kfree_skb_reason()")&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Using this commit and later additions, kernel developers are now able to specify why a given packet dropped. In the following example, a packet is dropped because no socket was found:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;- kfree_skb(skb); + kfree_skb_reason(skb, SKB_DROP_REASON_NO_SOCKET);&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Using tools to retrieve drop reasons&lt;/h2&gt; &lt;p&gt;The SKB drop reason can be retrieved in a few different ways, depending on which you are comfortable using, what is available on a given system, and the end goal (some solutions have more flexibility than others).&lt;/p&gt; &lt;p&gt;The main interface to retrieve the drop reason is the &lt;code&gt;skb:kfree_skb&lt;/code&gt; tracepoint. It provides a user readable text for all drop reasons. A good way to attach to this tracepoint is to use &lt;code&gt;perf&lt;/code&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# perf record -e skb:kfree_skb curl https://localhost # given no server listens on localhost:443/tcp. # perf script curl 883 [001] 340.799805: skb:kfree_skb: skbaddr=0xffff88811f6a7068 protocol=2048 location=tcp_v4_rcv+0x157 reason: NO_SOCKET curl 883 [001] 340.800860: skb:kfree_skb: skbaddr=0xffff88811f6a6de8 protocol=34525 location=tcp_v6_rcv+0x137 reason: NO_SOCKET&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can see why the two packets where dropped in &lt;code&gt;tcp_v4_rcv&lt;/code&gt; and &lt;code&gt;tcp_v6_rcv&lt;/code&gt; because no socket was found and we do not have a server listening on &lt;code&gt;localhost:443/tcp&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;We can also use other tools such as &lt;code&gt;bpftrace&lt;/code&gt; to get the drop reason, which would give us more flexibility, the drawback being the reason isn't converted to a human readable string:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# bpftrace -e 'tracepoint:skb:kfree_skb {printf("%s: %d\n", comm, args-&gt;reason)}' -c 'curl https://localhost' Attaching 1 probe... curl: 3 curl: 3 curl: (7) Failed to connect to localhost port 443 after 2 ms: Couldn't connect to server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Another method is the &lt;code&gt;dropwatch&lt;/code&gt;, an interactive tool to monitor packets dropped in the Linux kernel. When using the &lt;code&gt;packet alert mode&lt;/code&gt;, drop reasons are included.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# dropwatch -l kas Initializing kallsyms db dropwatch&gt; set alertmode packet Setting alert mode Alert mode successfully set dropwatch&gt; start Enabling monitoring... Kernel monitoring activated. Issue Ctrl-C to stop monitoring drop at: tcp_v4_rcv+0x157/0x1630 (0xffffffff8abc4f87) origin: software input port ifindex: 1 timestamp: Thu Feb 23 18:03:36 2023 370138884 nsec protocol: 0x800 length: 74 original length: 74 drop reason: NO_SOCKET drop at: tcp_v6_rcv+0x137/0x14f0 (0xffffffff8ad91b37) origin: software input port ifindex: 1 timestamp: Thu Feb 23 18:03:36 2023 372335338 nsec protocol: 0x86dd length: 94 original length: 94 drop reason: NO_SOCKET&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;The &lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/net/dropreason-core.h?h=v6.4-rc1#n88"&gt;&lt;code&gt;skb_drop_reason enum&lt;/code&gt;&lt;/a&gt; defines core drop reasons. It is an internal definition, and the actual value of all its members is not guaranteed to be constant over time. This feature is recent and some of the drop reasons were reordered during development. There is also work ongoing for supporting drop reasons from different subsystems. You should either use tools directly providing the drop reason in a text format (perf or dropwatch) or take the right drop reasons definition as a reference when retrieving the drop reason in a numeric way (bpftrace).&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Not all drop places in the Linux kernel are covered. Converting them to this new facility takes time and resources. There is progress upstream with more additions. Currently, more than &lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/include/net/dropreason.h?h=v6.2#n81"&gt;70 reasons are supported&lt;/a&gt;. There is also an effort to support more than the core networking subsystem.&lt;/p&gt; &lt;p&gt;SKB drop reasons are now available in &lt;a href="https://developers.redhat.com/topics/linux"&gt;Red Hat Enterprise Linux&lt;/a&gt; starting with RHEL 8.8 and RHEL 9.2.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/19/how-retrieve-packet-drop-reasons-linux-kernel" title="How to retrieve packet drop reasons in the Linux kernel"&gt;How to retrieve packet drop reasons in the Linux kernel&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Antoine Tenart</dc:creator><dc:date>2023-07-19T07:00:00Z</dc:date></entry><entry><title type="html">Top 20 Must-Read Software Trends Reports for 2023</title><link rel="alternate" href="http://www.ofbizian.com/2023/07/top-20-must-read-software-reports.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2023/07/top-20-must-read-software-reports.html</id><updated>2023-07-18T23:23:00Z</updated><content type="html">In the rapidly evolving software industry, keeping up with new trends, tools, and best practices can be time-consuming. With so much information available, where do you start, and what sources can you trust? I've curated a list of reports that I follow to stay informed and ahead of the curve. These provide insights into everything from programming languages to DevOps, cloud strategy, and security. If you're interested in the latest trends and fascinating posts I come across,  or check out my latest writing on industry trends over at the . I share anything I find insightful and worth reading in the world of cloud and distributed systems.  Here are the top 20 reports for 2023 I came across so far: ReportPublisherTIOBERedMonkStack OverflowInfoQInfoQPostmanDataDogThoughtworksO'ReillyHashiCorpRed HatPuppetLabsVmwareDenoAirByteDatabricksGithubRapidAPICNCFGoogle While these reports offer valuable insights, it's important to keep in mind that they can be opinionated. The key to effectively leveraging these resources lies in cross-verifying trends from multiple sources and using them only as a guide for direction rather than absolute truths. Are there any reports that should be on this list? Tag me on Twitter and I'll include them, subject to my checks I'm always keen to explore new sources! Found this list helpful? Go ahead, Call to action: Are you a user? Your experience is valuable! Contribute your insights and shape the</content><dc:creator>Unknown</dc:creator></entry><entry><title type="html">Keycloak 22.0.1 released</title><link rel="alternate" href="https://www.keycloak.org/2023/07/keycloak-2201-released" /><author><name /></author><id>https://www.keycloak.org/2023/07/keycloak-2201-released</id><updated>2023-07-18T00:00:00Z</updated><content type="html">To download the release go to . MIGRATION FROM 21.1 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. ALL RESOLVED ISSUES ENHANCEMENTS * Revisit Pod-Template in Keycloak CR keycloak operator * Support configurable custom Identity Providers keycloak * [REG 21-&gt;22] Error messages on kc build keycloak dist/quarkus BUGS * Accessibility/Clients List: Minor Issues keycloak admin/ui * `keycloakCRName` and `realm` are no longer marked as required in KeycloakRealmImport CRD keycloak operator * Version 22.0.0 not started in dev mode and build mode keycloak dist/quarkus * Migration for 22.0.0 is missing from the documentation keycloak docs * Broken links to quickstarts in documentation keycloak docs * Account V3 Missing translate Refresh keycloak account/ui * Keycloak is storing error events even if storing events is disabled keycloak storage * Fixing broken JSON translation files keycloak admin/ui UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry></feed>
